{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from app.core.config import settings\n",
    "\n",
    "from app.assistant.constant import (\n",
    "    CONTEXT_KEY,\n",
    "    EBP_ACCESS_TOKEN_KEY,\n",
    "    EBP_COOKIE_KEY,\n",
    "    EBP_EDGE_DOMAIN_KEY,\n",
    "    LLM_MODEL_KEY,\n",
    "    THREAD_ID_KEY,\n",
    ")\n",
    "\n",
    "from app.core.context import RequestContext\n",
    "\n",
    "\n",
    "def get_config(thread_id):\n",
    "    config = RunnableConfig(\n",
    "        configurable={\n",
    "            THREAD_ID_KEY: thread_id,\n",
    "            CONTEXT_KEY: RequestContext(thread_id),\n",
    "            LLM_MODEL_KEY: AzureChatOpenAI(\n",
    "                azure_deployment=settings.azure_openai_deployment\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x308835090>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x30f8175d0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x3108ba490>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x3108a4a50>, deployment_name='gpt-4-32k-september')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.assistant_v2.constant import CONFIGURABLE_CONTEXT_KEY\n",
    "\n",
    "\n",
    "THREAD_ID = \"test_1\"\n",
    "model = get_config(thread_id=THREAD_ID).get(CONFIGURABLE_CONTEXT_KEY).get(LLM_MODEL_KEY)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools.structured import StructuredTool\n",
    "\n",
    "\n",
    "class EvaluateResultInput(BaseModel):\n",
    "    is_a_good_answer: bool = Field(\n",
    "        description=\"Whether the last answer is match with the instruction in the given context or not (True/False)\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"The reason why the last answer is match with the instruction in the given context or not\"\n",
    "    )\n",
    "    guidance_for_good_answer: Optional[str] = Field(\n",
    "        description=\"If the last answer is not a good answer, generate this guidance for making a good answer which help to match with the instruction in the given context\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def _evaluate_result(\n",
    "    is_a_good_answer: bool, reason: str, guidance_for_good_answer: Optional[str]\n",
    "):\n",
    "    return EvaluateResultInput(\n",
    "        is_a_good_answer=is_a_good_answer,\n",
    "        reason=reason,\n",
    "        guidance_for_good_answer=guidance_for_good_answer,\n",
    "    ).json()\n",
    "\n",
    "\n",
    "evaluate_result_tool: StructuredTool = StructuredTool.from_function(\n",
    "    coroutine=_evaluate_result,\n",
    "    name=\"EvaluateResult\",\n",
    "    description=\"Call this tool to provide the evaluation result of the last answer to the system\",\n",
    "    args_schema=EvaluateResultInput,\n",
    "    error_on_invalid_docstring=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from app.assistant_v2.constant import CONFIGURABLE_CONTEXT_KEY\n",
    "from app.assistant_v2.transfer.state import TransferAgentState, TransferAgentStateFields\n",
    "\n",
    "\n",
    "async def reflection_fnc(\n",
    "    state: TransferAgentState, config: RunnableConfig\n",
    ") -> dict[str, any]:\n",
    "    prompt = \"\"\"\n",
    "    You are a expert judge in the field of AI response verification. Your task is to evaluate whether the AI response is correct or not based on the instruction prompt.\n",
    "    You are provided with a original instruction prompt for the model and a conversation history. You need to evaluate the last response of the model and call tool named EvaluateResult to generate resutl.\n",
    "    The result contains the following fields:\n",
    "    - is_a_good_answer: Whether the last answer is match with the instruction in the given context or not (True/False)\n",
    "    - reason: The reason why the last answer is match with the instruction in the given context or not\n",
    "    - guidance_for_good_answer: If the last answer is not a good answer, generate a guidance for making a good answer which help to match with the instruction in the given context\n",
    "    Here is the original instruction prompt: {{original_instruction_prompt}}\n",
    "    Here is the conversation history: {{conversation_history}}\n",
    "    \"\"\"\n",
    "    prev_prompt = config.get(CONFIGURABLE_CONTEXT_KEY)\n",
    "    messages = state[TransferAgentStateFields.MESSAGES]\n",
    "\n",
    "    prompt_tmpl = ChatPromptTemplate.from_messages([(\"system\", prompt)])\n",
    "    chain = prompt_tmpl | model.bind_tools([evaluate_result_tool])\n",
    "    response = await chain.ainvoke(\n",
    "        {\"original_instruction_prompt\": prev_prompt, \"conversation_history\": messages}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
